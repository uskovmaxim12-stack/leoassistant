"""
–°–∞–º–æ–æ–±—É—á–∞–µ–º–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å Leo Assistant
"""
import json
import pickle
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
import torch
import torch.nn as nn
import torch.optim as optim
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from config import config
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class KnowledgeMemory:
    """–ü–∞–º—è—Ç—å –∑–Ω–∞–Ω–∏–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
    
    def __init__(self, knowledge_base_path: str = None):
        self.knowledge_base_path = knowledge_base_path or config.KNOWLEDGE_BASE_PATH
        Path(self.knowledge_base_path).mkdir(parents=True, exist_ok=True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        
        # –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –≤ –ø–∞–º—è—Ç–∏
        self.knowledge_base: List[Dict] = []
        self.embeddings = None
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è
        self.load_base_knowledge()
    
    def load_base_knowledge(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π (–∏–∑–Ω–∞—á–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)"""
        base_knowledge = [
            {
                "question": "–ø—Ä–∏–≤–µ—Ç",
                "answer": "–ü—Ä–∏–≤–µ—Ç! –Ø –õ–µ–æ, —Ç–≤–æ–π —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –≤ —É—á–µ–±–µ. –†–∞–¥ —Ç–µ–±—è –≤–∏–¥–µ—Ç—å!",
                "context": ["–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ"],
                "score": 1.0
            },
            {
                "question": "–∫–∞–∫ –¥–µ–ª–∞",
                "answer": "–£ –º–µ–Ω—è –≤—Å–µ –æ—Ç–ª–∏—á–Ω–æ! –ì–æ—Ç–æ–≤ –ø–æ–º–æ–≥–∞—Ç—å —Ç–µ–±–µ —Å —É—á–µ–±–æ–π. –ö–∞–∫ —Ç–≤–æ–∏ —É—Å–ø–µ—Ö–∏?",
                "context": ["—Ä–∞–∑–≥–æ–≤–æ—Ä"],
                "score": 1.0
            },
            {
                "question": "–ø–æ–º–æ—â—å",
                "answer": "–Ø –º–æ–≥—É –ø–æ–º–æ—á—å —Å —É—á–µ–±–æ–π, –æ–±—ä—è—Å–Ω–∏—Ç—å —Ç–µ–º—É, –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–∞–¥–∞–Ω–∏–µ –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø–æ–±–æ–ª—Ç–∞—Ç—å. –ß—Ç–æ —Ç–µ–±—è –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç?",
                "context": ["–ø–æ–º–æ—â—å"],
                "score": 1.0
            },
            # –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ 7 –∫–ª–∞—Å—Å
            {
                "question": "—á—Ç–æ —Ç–∞–∫–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ",
                "answer": "–£—Ä–∞–≤–Ω–µ–Ω–∏–µ ‚Äî —ç—Ç–æ —Ä–∞–≤–µ–Ω—Å—Ç–≤–æ, —Å–æ–¥–µ—Ä–∂–∞—â–µ–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é, –∑–Ω–∞—á–µ–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏. –ù–∞–ø—Ä–∏–º–µ—Ä: 2x + 3 = 11",
                "context": ["–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "–∞–ª–≥–µ–±—Ä–∞", "7 –∫–ª–∞—Å—Å"],
                "score": 1.0
            },
            {
                "question": "–∫–∞–∫ —Ä–µ—à–∞—Ç—å –ª–∏–Ω–µ–π–Ω—ã–µ —É—Ä–∞–≤–Ω–µ–Ω–∏—è",
                "answer": "–õ–∏–Ω–µ–π–Ω—ã–µ —É—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–µ—à–∞—é—Ç—Å—è —Ç–∞–∫:\n1. –ü–µ—Ä–µ–Ω–µ—Å—Ç–∏ –≤—Å–µ —Å x –≤ –æ–¥–Ω—É —Å—Ç–æ—Ä–æ–Ω—É, —á–∏—Å–ª–∞ –≤ –¥—Ä—É–≥—É—é\n2. –£–ø—Ä–æ—Å—Ç–∏—Ç—å\n3. –†–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø—Ä–∏ x\n–ü—Ä–∏–º–µ—Ä: 3x - 7 = 8 ‚Üí 3x = 15 ‚Üí x = 5",
                "context": ["–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "–∞–ª–≥–µ–±—Ä–∞", "7 –∫–ª–∞—Å—Å"],
                "score": 1.0
            },
            # –†—É—Å—Å–∫–∏–π —è–∑—ã–∫
            {
                "question": "—á—Ç–æ —Ç–∞–∫–æ–µ –ø—Ä–∏—á–∞—Å—Ç–∏–µ",
                "answer": "–ü—Ä–∏—á–∞—Å—Ç–∏–µ ‚Äî —ç—Ç–æ –æ—Å–æ–±–∞—è —Ñ–æ—Ä–º–∞ –≥–ª–∞–≥–æ–ª–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–±–æ–∑–Ω–∞—á–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫ –ø—Ä–µ–¥–º–µ—Ç–∞ –ø–æ –¥–µ–π—Å—Ç–≤–∏—é. –û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã: –∫–∞–∫–æ–π? —á—Ç–æ –¥–µ–ª–∞—é—â–∏–π? —á—Ç–æ —Å–¥–µ–ª–∞–≤—à–∏–π?",
                "context": ["—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫", "–≥—Ä–∞–º–º–∞—Ç–∏–∫–∞", "7 –∫–ª–∞—Å—Å"],
                "score": 1.0
            },
            # –ì–µ–æ–º–µ—Ç—Ä–∏—è
            {
                "question": "—Ç–µ–æ—Ä–µ–º–∞ –ø–∏—Ñ–∞–≥–æ—Ä–∞",
                "answer": "–¢–µ–æ—Ä–µ–º–∞ –ü–∏—Ñ–∞–≥–æ—Ä–∞: –≤ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–æ–º —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–µ –∫–≤–∞–¥—Ä–∞—Ç –≥–∏–ø–æ—Ç–µ–Ω—É–∑—ã —Ä–∞–≤–µ–Ω —Å—É–º–º–µ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –∫–∞—Ç–µ—Ç–æ–≤. –§–æ—Ä–º—É–ª–∞: c¬≤ = a¬≤ + b¬≤",
                "context": ["–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "–≥–µ–æ–º–µ—Ç—Ä–∏—è", "7 –∫–ª–∞—Å—Å"],
                "score": 1.0
            },
            # –û–±—â–∏–µ –∑–Ω–∞–Ω–∏—è
            {
                "question": "–∫—Ç–æ —Ç–∞–∫–æ–π –ª–µ–æ",
                "answer": "–Ø ‚Äî –õ–µ–æ, –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Å–æ–∑–¥–∞–Ω–Ω—ã–π —á—Ç–æ–±—ã –ø–æ–º–æ–≥–∞—Ç—å —É—á–µ–Ω–∏–∫–∞–º 7–ë –∫–ª–∞—Å—Å–∞ –≤ —É—á–µ–±–µ. –Ø —É–º–µ—é –æ–±—ä—è—Å–Ω—è—Ç—å —Ç–µ–º—ã, –ø—Ä–æ–≤–µ—Ä—è—Ç—å –∑–∞–¥–∞–Ω–∏—è –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä!",
                "context": ["–æ —Å–µ–±–µ"],
                "score": 1.0
            }
        ]
        
        self.knowledge_base.extend(base_knowledge)
        self.update_embeddings()
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(base_knowledge)} –±–∞–∑–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π")
    
    def update_embeddings(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        if self.knowledge_base:
            texts = [f"{item['question']} {''.join(item.get('context', []))}" 
                    for item in self.knowledge_base]
            self.embeddings = self.embedding_model.encode(texts)
    
    def add_knowledge(self, question: str, answer: str, context: List[str] = None, 
                      source: str = "user"):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∑–Ω–∞–Ω–∏—è"""
        new_item = {
            "question": question.lower(),
            "answer": answer,
            "context": context or [],
            "source": source,
            "score": 1.0,
            "added_at": datetime.now().isoformat(),
            "times_used": 0
        }
        
        self.knowledge_base.append(new_item)
        self.update_embeddings()
        self.save_knowledge()
        logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤–æ–µ –∑–Ω–∞–Ω–∏–µ: {question[:50]}...")
    
    def find_similar(self, query: str, top_k: int = 3) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–Ω–∞–Ω–∏–π"""
        if not self.knowledge_base:
            return []
        
        query_embedding = self.embedding_model.encode([query])
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã top_k –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.3:  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
                item = self.knowledge_base[idx].copy()
                item["similarity"] = float(similarities[idx])
                results.append(item)
        
        return results
    
    def save_knowledge(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –≤ —Ñ–∞–π–ª"""
        file_path = Path(self.knowledge_base_path) / "knowledge_base.json"
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(self.knowledge_base, f, ensure_ascii=False, indent=2)
        logger.info(f"–ó–Ω–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {file_path}")
    
    def load_knowledge(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∑–Ω–∞–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–∞"""
        file_path = Path(self.knowledge_base_path) / "knowledge_base.json"
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                self.knowledge_base = json.load(f)
            self.update_embeddings()
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.knowledge_base)} –∑–Ω–∞–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–∞")
            return True
        return False


class NeuralNetworkModel(nn.Module):
    """–ú–æ–¥–µ–ª—å –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤"""
    
    def __init__(self, vocab_size: int = 10000, embedding_dim: int = 256, 
                 hidden_dim: int = 512):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, 
                           num_layers=2, dropout=0.3)
        self.fc = nn.Linear(hidden_dim, vocab_size)
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x, hidden=None):
        embedded = self.embedding(x)
        output, hidden = self.lstm(embedded, hidden)
        output = self.dropout(output)
        logits = self.fc(output)
        return logits, hidden


class LeoNeuralNetwork:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ Leo Assistant"""
    
    def __init__(self):
        self.model = None
        self.memory = KnowledgeMemory()
        self.vocab = {}
        self.reverse_vocab = {}
        self.is_trained = False
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—Å—Ç—å
        self.load_model()
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        model_path = Path(config.NEURAL_NETWORK_MODEL_PATH)
        
        if model_path.exists():
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Å–ª–æ–≤–∞—Ä—å
                checkpoint = torch.load(model_path, map_location='cpu')
                self.model = NeuralNetworkModel()
                self.model.load_state_dict(checkpoint['model_state'])
                self.vocab = checkpoint['vocab']
                self.reverse_vocab = {v: k for k, v in self.vocab.items()}
                self.is_trained = checkpoint.get('is_trained', False)
                
                logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}")
                return True
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª–∏ –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é
        self.initialize_vocab()
        self.model = NeuralNetworkModel(vocab_size=len(self.vocab))
        logger.info("–°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å")
        return False
    
    def initialize_vocab(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤–∞—Ä—è"""
        # –ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å
        base_words = [
            '–ø—Ä–∏–≤–µ—Ç', '–ø–æ–∫–∞', '–∫–∞–∫', '–¥–µ–ª–∞', '–ø–æ–º–æ—â—å', '—É—á–µ–±–∞', '–∑–∞–¥–∞–Ω–∏–µ',
            '—É—Ä–æ–∫', '—É—á–∏—Ç–µ–ª—å', '—É—á–µ–Ω–∏–∫', '—à–∫–æ–ª–∞', '–∫–ª–∞—Å—Å', '–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞',
            '—Ä—É—Å—Å–∫–∏–π', '—è–∑—ã–∫', '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', '–∏—Å—Ç–æ—Ä–∏—è', '–≥–µ–æ–≥—Ä–∞—Ñ–∏—è',
            '—Ñ–∏–∑–∏–∫–∞', '—Ö–∏–º–∏—è', '–æ—Ç–≤–µ—Ç', '–≤–æ–ø—Ä–æ—Å', '–æ–±—ä—è—Å–Ω–∏', '—Ä–µ—à–∏',
            '–ø—Ä–∏–º–µ—Ä', '–∑–∞–¥–∞—á–∞', '—Ç–µ–æ—Ä–∏—è', '–ø—Ä–∞–∫—Ç–∏–∫–∞', '—Ç–µ—Å—Ç', '—ç–∫–∑–∞–º–µ–Ω',
            '–æ—Ü–µ–Ω–∫–∞', '–¥–æ–º–∞—à–Ω–µ–µ', '—Ä–∞–±–æ—Ç–∞', '–ø—Ä–æ–µ–∫—Ç', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ',
            '—Å–ø–∞—Å–∏–±–æ', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞', '–∏–∑–≤–∏–Ω–∏', '–ø–æ–Ω—è—Ç–Ω–æ', '–Ω–µ–ø–æ–Ω—è—Ç–Ω–æ'
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ –≤ —Å–ª–æ–≤–∞—Ä—å
        self.vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}
        for idx, word in enumerate(base_words, start=4):
            self.vocab[word] = idx
        
        self.reverse_vocab = {v: k for k, v in self.vocab.items()}
    
    def text_to_tensor(self, text: str) -> torch.Tensor:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–µ–Ω–∑–æ—Ä"""
        words = text.lower().split()
        indices = [self.vocab.get(word, self.vocab['<UNK>']) for word in words]
        return torch.tensor(indices).unsqueeze(0)
    
    def generate_response(self, query: str, context: List[Dict] = None) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å
        1. –ò—â–µ–º –≤ –ø–∞–º—è—Ç–∏ –∑–Ω–∞–Ω–∏–π
        2. –ï—Å–ª–∏ –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é
        3. –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        """
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –≤–æ–ø—Ä–æ—Å—ã –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        similar = self.memory.find_similar(query)
        
        if similar and similar[0]['similarity'] > 0.7:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
            best_match = similar[0]
            best_match['times_used'] = best_match.get('times_used', 0) + 1
            
            # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –ø–æ–≤—ã—à–∞–µ–º –µ–≥–æ score
            if best_match['times_used'] > 10:
                best_match['score'] = min(1.0, best_match.get('score', 0) + 0.1)
            
            logger.info(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –æ—Ç–≤–µ—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (—Å—Ö–æ–¥—Å—Ç–≤–æ: {best_match['similarity']:.2f})")
            return best_match['answer']
        
        # –ï—Å–ª–∏ –≤ –±–∞–∑–µ –Ω–µ—Ç —Ö–æ—Ä–æ—à–µ–≥–æ –æ—Ç–≤–µ—Ç–∞, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º
        if self.is_trained:
            # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç—å
            # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª–æ-–æ—Å–Ω–æ–≤—É
            response = self.generate_based_on_rules(query)
        else:
            # –ù–µ–π—Ä–æ—Å–µ—Ç—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª–∞
            response = self.generate_based_on_rules(query)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –¥–ª—è –±—É–¥—É—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        context_tags = self.extract_context(query, context)
        self.memory.add_knowledge(query, response, context_tags, source="generated")
        
        return response
    
    def generate_based_on_rules(self, query: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª"""
        query_lower = query.lower()
        
        # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        rules = {
            '–∫–∞–∫ —Ç–µ–±—è –∑–æ–≤—É—Ç': '–ú–µ–Ω—è –∑–æ–≤—É—Ç –õ–µ–æ! –Ø —Ç–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫ –≤ —É—á–µ–±–µ.',
            '—Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏': f'–°–µ–π—á–∞—Å {datetime.now().strftime("%H:%M")}. –í—Ä–µ–º—è —É—á–∏—Ç—å—Å—è!',
            '–∫–∞–∫–∞—è —Å–µ–≥–æ–¥–Ω—è –¥–∞—Ç–∞': f'–°–µ–≥–æ–¥–Ω—è {datetime.now().strftime("%d.%m.%Y")}.',
            '—Å–ø–∞—Å–∏–±–æ': '–í—Å–µ–≥–¥–∞ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞! –†–∞–¥ –±—ã–ª –ø–æ–º–æ—á—å.',
            '–ø–æ–∫–∞': '–î–æ –≤—Å—Ç—Ä–µ—á–∏! –ù–µ –∑–∞–±—ã–≤–∞–π –¥–µ–ª–∞—Ç—å –¥–æ–º–∞—à–Ω–∏–µ –∑–∞–¥–∞–Ω–∏—è!',
            '—É—Ä–∞': '–£—Ä–∞! üéâ –¢—ã –º–æ–ª–æ–¥–µ—Ü! –ü—Ä–æ–¥–æ–ª–∂–∞–π –≤ —Ç–æ–º –∂–µ –¥—É—Ö–µ!',
            '—Å–ª–æ–∂–Ω–æ': '–ù–µ –ø–µ—Ä–µ–∂–∏–≤–∞–π! –í—Å–µ —Å–ª–æ–∂–Ω—ã–µ —Ç–µ–º—ã –º–æ–∂–Ω–æ –æ—Å–≤–æ–∏—Ç—å –ø–æ —à–∞–≥–∞–º. –î–∞–≤–∞–π —Ä–∞–∑–±–µ—Ä–µ–º –≤–º–µ—Å—Ç–µ?',
            '—Å–∫—É—á–Ω–æ': '–î–∞–≤–∞–π —Å–¥–µ–ª–∞–µ–º —É—á–µ–±—É –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–π! –•–æ—á–µ—à—å –ø–æ–∏–≥—Ä–∞—Ç—å –≤ —É—á–µ–±–Ω—É—é –∏–≥—Ä—É?',
            '—É—Å—Ç–∞–ª': '–û—Ç–¥–æ—Ö–Ω–∏ –Ω–µ–º–Ω–æ–≥–æ! –ü–µ—Ä–µ—Ä—ã–≤ –≤–∞–∂–µ–Ω –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ–π —É—á–µ–±—ã.',
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª–∞
        for pattern, response in rules.items():
            if pattern in query_lower:
                return response
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–∞–≤–∏–ª–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π –æ—Ç–≤–µ—Ç
        responses = [
            "–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –≤–æ–ø—Ä–æ—Å! –î–∞–≤–∞–π —Ä–∞–∑–±–µ—Ä–µ–º –µ–≥–æ –≤–º–µ—Å—Ç–µ.",
            "–•–º, —Ö–æ—Ä–æ—à–∏–π –≤–æ–ø—Ä–æ—Å. –ú–Ω–µ –Ω—É–∂–Ω–æ –ø–æ–¥—É–º–∞—Ç—å –Ω–∞–¥ –æ—Ç–≤–µ—Ç–æ–º.",
            "–Ø –µ—â–µ —É—á—É—Å—å, –Ω–æ –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –ø–æ–º–æ—á—å! –ú–æ–∂–µ—à—å –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å?",
            "–≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —É—á–µ–±–æ–π? –Ø —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Å—å –Ω–∞ —à–∫–æ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–º–µ—Ç–∞—Ö 7 –∫–ª–∞—Å—Å–∞.",
            "–ü–æ–∫–∞ —è –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø—Ä–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫—É, —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ –∏ –¥—Ä—É–≥–∏–µ —à–∫–æ–ª—å–Ω—ã–µ –ø—Ä–µ–¥–º–µ—Ç—ã.",
        ]
        
        import random
        return random.choice(responses)
    
    def extract_context(self, query: str, context: List[Dict] = None) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –∏ –∏—Å—Ç–æ—Ä–∏–∏"""
        context_tags = []
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ç–µ–≥–æ–≤
        subjects = ['–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞', '–∞–ª–≥–µ–±—Ä–∞', '–≥–µ–æ–º–µ—Ç—Ä–∏—è', '—Ä—É—Å—Å–∫–∏–π', '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', 
                   '–∏—Å—Ç–æ—Ä–∏—è', '–≥–µ–æ–≥—Ä–∞—Ñ–∏—è', '—Ñ–∏–∑–∏–∫–∞', '—Ö–∏–º–∏—è', '–±–∏–æ–ª–æ–≥–∏—è', '–∞–Ω–≥–ª–∏–π—Å–∫–∏–π']
        
        query_lower = query.lower()
        for subject in subjects:
            if subject in query_lower:
                context_tags.append(subject)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–µ —Ç–µ–≥–∏
        if any(word in query_lower for word in ['–∫–∞–∫', '–ø–æ—á–µ–º—É', '–æ–±—ä—è—Å–Ω–∏']):
            context_tags.append('–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ')
        
        if any(word in query_lower for word in ['–∑–∞–¥–∞—á–∞', '–ø—Ä–∏–º–µ—Ä', '—Ä–µ—à–∏']):
            context_tags.append('–∑–∞–¥–∞—á–∞')
        
        if any(word in query_lower for word in ['–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ', '—á—Ç–æ —Ç–∞–∫–æ–µ']):
            context_tags.append('–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ')
        
        return context_tags if context_tags else ['–æ–±—â–∏–π']
    
    def train_on_data(self, data: List[Dict], epochs: int = 10):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö"""
        if not data:
            logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è
        optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()
        
        logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {len(data)} –ø—Ä–∏–º–µ—Ä–∞—Ö")
        
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–µ–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –æ—Ç–º–µ—á–∞–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å "–æ–±—É—á–µ–Ω–∞"
        self.is_trained = True
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        self.save_model()
        logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        model_path = Path(config.NEURAL_NETWORK_MODEL_PATH)
        model_path.parent.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'model_state': self.model.state_dict(),
            'vocab': self.vocab,
            'is_trained': self.is_trained,
            'saved_at': datetime.now().isoformat()
        }
        
        torch.save(checkpoint, model_path)
        logger.info(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
    
    def learn_from_conversation(self, user_message: str, ai_response: str, 
                               was_helpful: bool = True):
        """
        –û–±—É—á–µ–Ω–∏–µ –∏–∑ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        was_helpful: –±—ã–ª –ª–∏ –æ—Ç–≤–µ—Ç –ø–æ–ª–µ–∑–µ–Ω (–¥–ª—è reinforcement learning)
        """
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        context = self.extract_context(user_message)
        self.memory.add_knowledge(user_message, ai_response, context, source="chat")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º score –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏
        if was_helpful:
            # –ü–æ–≤—ã—à–∞–µ–º score –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø–∏—Å–µ–π
            similar = self.memory.find_similar(user_message)
            for item in similar[:3]:
                item['score'] = min(1.0, item.get('score', 0) + 0.05)
        else:
            # –ü–æ–Ω–∏–∂–∞–µ–º score
            similar = self.memory.find_similar(user_message)
            for item in similar[:3]:
                item['score'] = max(0.1, item.get('score', 1.0) - 0.1)
        
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        if len(self.memory.knowledge_base) % 50 == 0:
            logger.info("–ù–∞–∫–æ–ø–ª–µ–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è")
            # TODO: –ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    
    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
        return {
            "knowledge_base_size": len(self.memory.knowledge_base),
            "is_trained": self.is_trained,
            "vocab_size": len(self.vocab),
            "model_parameters": sum(p.numel() for p in self.model.parameters()) if self.model else 0,
            "last_updated": datetime.now().isoformat()
        }


# –°–∏–Ω–≥–ª—Ç–æ–Ω –∏–Ω—Å—Ç–∞–Ω—Å –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
_leo_nn_instance = None

def get_neural_network() -> LeoNeuralNetwork:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Å—Ç–∞–Ω—Å–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (—Å–∏–Ω–≥–ª—Ç–æ–Ω)"""
    global _leo_nn_instance
    if _leo_nn_instance is None:
        _leo_nn_instance = LeoNeuralNetwork()
    return _leo_nn_instance
